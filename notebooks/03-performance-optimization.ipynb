{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @robotodio\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "detoxify    NA\n",
      "matplotlib  3.3.2\n",
      "numpy       1.19.2\n",
      "pandas      1.2.0\n",
      "seaborn     0.11.1\n",
      "sinfo       0.3.1\n",
      "tweepy      3.10.0\n",
      "-----\n",
      "IPython             7.19.0\n",
      "jupyter_client      6.1.7\n",
      "jupyter_core        4.7.0\n",
      "jupyterlab          2.2.6\n",
      "notebook            6.1.6\n",
      "-----\n",
      "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Windows-10-10.0.19041-SP0\n",
      "4 logical CPU cores, Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\n",
      "-----\n",
      "Session information updated at 2021-03-23 18:15\n"
     ]
    }
   ],
   "source": [
    "# Python libraries\n",
    "# ------------------------------------------------------------------------------\n",
    "# Enviroment\n",
    "import os\n",
    "from sinfo import sinfo\n",
    "\n",
    "# Performance\n",
    "from time import time\n",
    "\n",
    "# Reading files with different formats\n",
    "import json\n",
    "\n",
    "# Data wrangling\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualitation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Twitter API\n",
    "import tweepy\n",
    "\n",
    "# Hate speech detection\n",
    "from detoxify import Detoxify\n",
    "\n",
    "sinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this notebook is to obtain the optimal number of tweets to obtain a representative sample of data, without affecting the performance of the code.\n",
    "\n",
    "To analyze this, the functional part of the code will be replicated within a for loop, which will be executed for different numbers of items downloaded and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Twitter credentials\n",
    "# ------------------------------------------------------------------------------\n",
    "# Open .json file containing credentials/tokens as a dictionary\n",
    "with open(\"../twitter_api_keys.json\") as file:\n",
    "    api_credentials = json.load(file)\n",
    "    \n",
    "# Assign each value of the dictionary to a new variable\n",
    "consumer_key = api_credentials['consumer_key']\n",
    "consumer_secret = api_credentials['consumer_secret']\n",
    "access_token = api_credentials['access_token']\n",
    "access_token_secret = api_credentials['access_token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Logged In Successfully.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# API set up\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a handler instance with key and secret consumer, and pass the tokens\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "# Construct the API instance\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Check credentials\n",
    "if(api.verify_credentials):\n",
    "    print('-'*30)\n",
    "    print(\"Logged In Successfully.\")\n",
    "    print('-'*30)\n",
    "else:\n",
    "    print(\"Error -- Could not log in with your credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets list iterator\n",
    "# ------------------------------------------------------------------------------\n",
    "def tweets_iterator(target, n_items):\n",
    "    '''\n",
    "    Returns an iterator of tweets.\n",
    "\n",
    "        Parameters:\n",
    "            target (str): The user name of the Twitter account.\n",
    "            n_items (int): Number of tweets downloaded.\n",
    "\n",
    "        Returns:\n",
    "            tweets (ItemIterator): an iterator of tweets.\n",
    "    '''\n",
    "    # Instantiate the iterator\n",
    "    tweets = tweepy.Cursor(\n",
    "        api.user_timeline,\n",
    "        screen_name=target,\n",
    "        include_rts=False,\n",
    "        exclude_replies=False,\n",
    "        tweet_mode='extended').items(n_items)\n",
    "    \n",
    "    # Returns iterator\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are authenticated, and we have defined a function to download tweets, we run the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-12c4bd3f05d9>:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  all_columns = [np.array([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test10', 'scoring_average = 0.27288484579185024', 'execution_time = 10.773355722427368')\n",
      "('test50', 'scoring_average = 0.22765086456143763', 'execution_time = 18.008984088897705')\n",
      "('test100', 'scoring_average = 0.23138363264180953', 'execution_time = 28.813783168792725')\n",
      "('test250', 'scoring_average = 0.20244164427777286', 'execution_time = 59.9163134098053')\n",
      "('test500', 'scoring_average = 0.20519548592192585', 'execution_time = 111.97544741630554')\n",
      "('test750', 'scoring_average = 0.23731756798484518', 'execution_time = 202.84372735023499')\n",
      "('test1000', 'scoring_average = 0.25159539312068957', 'execution_time = 276.5613822937012')\n"
     ]
    }
   ],
   "source": [
    "target = 'DFD_74'\n",
    "\n",
    "for i, n_items in enumerate([10, 50, 100, 250, 500, 750, 1000]):\n",
    "    \n",
    "    # Performance variables\n",
    "    initial_time = time() \n",
    "    test = 'test' + str(i)\n",
    "    \n",
    "    # Tweet extractor\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Tweets list (iterator)\n",
    "    tweets = tweets_iterator(target, n_items)\n",
    "    # Read through the iterator, and export the info to a Pandas DataFrame\n",
    "    all_columns = [np.array([\n",
    "        tweet.full_text,\n",
    "        tweet.user.screen_name,\n",
    "        tweet.id,\n",
    "        tweet.user.followers_count,\n",
    "        tweet.source,\n",
    "        tweet.created_at,\n",
    "        tweet.lang,\n",
    "        len(tweet.full_text),\n",
    "        tweet.favorite_count,\n",
    "        tweet.retweet_count,\n",
    "        re.findall(r\"#(\\w+)\", tweet.full_text)\n",
    "    ]) for tweet in tweets]\n",
    "    # Export the list of tweets to a dataframe\n",
    "    df = pd.DataFrame(\n",
    "        data=all_columns,\n",
    "        columns=['tweet', 'account', 'id', 'followers', 'source', 'date', 'language',\n",
    "                 'length', 'likes', 'RTs', 'hashtags'])\n",
    "    \n",
    "    \n",
    "    # Data cleaning\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Characters to remove\n",
    "    spec_chars = ['\\n', '\\t', '\\r']\n",
    "    # Replace defined characters with a whitespace\n",
    "    for char in spec_chars:\n",
    "        df['tweet'] = df['tweet'].str.strip().replace(char, ' ')\n",
    "    # Split and re-join each tweet\n",
    "    df['tweet'] = df['tweet'].str.split().str.join(\" \")\n",
    "    \n",
    "    \n",
    "    # Hate speech level prediction\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Returns a dictionary with toxicity values of each tweet. The key of the\n",
    "    # dictionary is called toxicity.\n",
    "    results = Detoxify('multilingual').predict(list(df['tweet']))\n",
    "    # Add the new info to the previous DataFrame\n",
    "    df['toxicity'] = results['toxicity']\n",
    "    # Define a class for each tweet\n",
    "    df['class'] = df['toxicity'].apply(lambda toxicity: 'toxic' if toxicity >= 0.5 else 'non-toxic')\n",
    "    # Mean scoring\n",
    "    scoring_average = df['toxicity'].mean()\n",
    "    \n",
    "    # Performance variables\n",
    "    final_time = time() \n",
    "    execution_time = final_time - initial_time\n",
    "    \n",
    "    results_tuple = ('test' + str(n_items),\n",
    "                     'scoring_average = ' + str(scoring_average),\n",
    "                     'execution_time = ' + str(execution_time))\n",
    "    \n",
    "    print(results_tuple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
