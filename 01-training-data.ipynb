{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @robotodio  -  training data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "# ------------------------------------------------------------------------------\n",
    "# Reading files with different formats\n",
    "import json\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 163.94 ms\n",
      "Type conversion took: 55.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 114.78 ms\n",
      "Type conversion took: 59.00 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 121.07 ms\n",
      "Type conversion took: 50.25 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 124.98 ms\n",
      "Type conversion took: 44.59 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 110.10 ms\n",
      "Type conversion took: 53.91 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 121.05 ms\n",
      "Type conversion took: 45.27 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.97 ms\n",
      "Type conversion took: 52.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 123.95 ms\n",
      "Type conversion took: 51.95 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 125.21 ms\n",
      "Type conversion took: 50.01 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 120.03 ms\n",
      "Type conversion took: 41.24 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.18 ms\n",
      "Type conversion took: 56.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 133.14 ms\n",
      "Type conversion took: 48.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 140.02 ms\n",
      "Type conversion took: 53.77 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 144.30 ms\n",
      "Type conversion took: 58.96 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 108.62 ms\n",
      "Type conversion took: 60.06 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 125.07 ms\n",
      "Type conversion took: 45.04 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 164.92 ms\n",
      "Type conversion took: 58.96 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 136.17 ms\n",
      "Type conversion took: 49.63 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.29 ms\n",
      "Type conversion took: 51.75 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 157.97 ms\n",
      "Type conversion took: 60.37 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 194.86 ms\n",
      "Type conversion took: 64.96 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 170.34 ms\n",
      "Type conversion took: 62.96 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 183.89 ms\n",
      "Type conversion took: 50.91 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 144.90 ms\n",
      "Type conversion took: 55.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 118.70 ms\n",
      "Type conversion took: 49.84 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 129.99 ms\n",
      "Type conversion took: 50.23 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 120.05 ms\n",
      "Type conversion took: 57.38 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 121.36 ms\n",
      "Type conversion took: 44.93 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 129.98 ms\n",
      "Type conversion took: 40.03 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.35 ms\n",
      "Type conversion took: 55.56 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 138.22 ms\n",
      "Type conversion took: 51.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 138.23 ms\n",
      "Type conversion took: 44.86 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 119.96 ms\n",
      "Type conversion took: 53.15 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 151.01 ms\n",
      "Type conversion took: 54.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 127.57 ms\n",
      "Type conversion took: 49.19 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 124.76 ms\n",
      "Type conversion took: 54.81 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 132.93 ms\n",
      "Type conversion took: 49.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 129.92 ms\n",
      "Type conversion took: 48.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 133.92 ms\n",
      "Type conversion took: 57.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 126.90 ms\n",
      "Type conversion took: 50.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.93 ms\n",
      "Type conversion took: 51.98 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 119.92 ms\n",
      "Type conversion took: 50.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 133.03 ms\n",
      "Type conversion took: 56.82 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 132.90 ms\n",
      "Type conversion took: 44.90 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 131.27 ms\n",
      "Type conversion took: 48.68 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.12 ms\n",
      "Type conversion took: 45.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 129.98 ms\n",
      "Type conversion took: 49.80 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 132.41 ms\n",
      "Type conversion took: 47.67 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.92 ms\n",
      "Type conversion took: 53.07 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 126.97 ms\n",
      "Type conversion took: 50.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 150.91 ms\n",
      "Type conversion took: 51.95 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.94 ms\n",
      "Type conversion took: 56.74 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.39 ms\n",
      "Type conversion took: 56.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 148.91 ms\n",
      "Type conversion took: 54.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 123.15 ms\n",
      "Type conversion took: 54.04 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 144.25 ms\n",
      "Type conversion took: 60.95 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.75 ms\n",
      "Type conversion took: 45.03 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.01 ms\n",
      "Type conversion took: 50.20 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 151.80 ms\n",
      "Type conversion took: 57.95 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 120.13 ms\n",
      "Type conversion took: 54.83 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 114.85 ms\n",
      "Type conversion took: 50.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 146.03 ms\n",
      "Type conversion took: 51.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 121.92 ms\n",
      "Type conversion took: 60.80 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 136.32 ms\n",
      "Type conversion took: 53.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 140.22 ms\n",
      "Type conversion took: 51.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.61 ms\n",
      "Type conversion took: 64.81 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 143.94 ms\n",
      "Type conversion took: 53.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 126.38 ms\n",
      "Type conversion took: 50.05 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 149.02 ms\n",
      "Type conversion took: 55.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 145.39 ms\n",
      "Type conversion took: 50.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.08 ms\n",
      "Type conversion took: 54.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 136.96 ms\n",
      "Type conversion took: 41.78 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 144.71 ms\n",
      "Type conversion took: 44.89 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.35 ms\n",
      "Type conversion took: 53.41 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 141.34 ms\n",
      "Type conversion took: 44.98 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.36 ms\n",
      "Type conversion took: 44.01 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.99 ms\n",
      "Type conversion took: 52.81 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 147.08 ms\n",
      "Type conversion took: 49.98 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 141.92 ms\n",
      "Type conversion took: 46.85 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 145.32 ms\n",
      "Type conversion took: 51.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 132.77 ms\n",
      "Type conversion took: 60.10 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 150.91 ms\n",
      "Type conversion took: 51.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 154.38 ms\n",
      "Type conversion took: 52.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 133.58 ms\n",
      "Type conversion took: 64.32 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 142.92 ms\n",
      "Type conversion took: 47.82 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 142.06 ms\n",
      "Type conversion took: 51.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 126.59 ms\n",
      "Type conversion took: 47.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 120.69 ms\n",
      "Type conversion took: 49.88 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.63 ms\n",
      "Type conversion took: 55.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 157.91 ms\n",
      "Type conversion took: 55.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 119.93 ms\n",
      "Type conversion took: 52.99 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 130.42 ms\n",
      "Type conversion took: 46.73 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 123.60 ms\n",
      "Type conversion took: 52.35 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 142.38 ms\n",
      "Type conversion took: 53.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 120.22 ms\n",
      "Type conversion took: 50.06 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 119.66 ms\n",
      "Type conversion took: 55.67 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 136.46 ms\n",
      "Type conversion took: 57.68 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 110.31 ms\n",
      "Type conversion took: 54.38 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 135.66 ms\n",
      "Type conversion took: 45.32 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 124.63 ms\n",
      "Type conversion took: 52.54 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 127.68 ms\n",
      "Type conversion took: 44.79 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 145.01 ms\n",
      "Type conversion took: 40.40 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 134.54 ms\n",
      "Type conversion took: 53.33 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 141.74 ms\n",
      "Type conversion took: 65.91 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 163.17 ms\n",
      "Type conversion took: 59.82 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 176.75 ms\n",
      "Type conversion took: 66.34 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 186.89 ms\n",
      "Type conversion took: 60.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 176.89 ms\n",
      "Type conversion took: 62.04 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 143.17 ms\n",
      "Type conversion took: 48.97 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 133.67 ms\n",
      "Type conversion took: 53.84 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 16.97 ms\n",
      "Type conversion took: 9.32 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  sexual_explicit  \n",
       "0         0.000000      0.0         0.000000  0.00000     0.0              0.0  \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0              0.0  \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0              0.0  \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0              0.0  \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0              0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path\n",
    "file = 'jubtc-train.csv.zip'\n",
    "# ------------------------------------------------------------------------------\n",
    "# Read in pandas\n",
    "df = pd.read_csv(\n",
    "    file,\n",
    "    header=0,\n",
    "    sep=',',\n",
    "    usecols=['id', 'comment_text', 'target', 'severe_toxicity', 'obscene',\n",
    "             'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
    "    compression='zip')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
